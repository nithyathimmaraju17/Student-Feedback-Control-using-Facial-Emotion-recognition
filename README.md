# Emotion_Detection_CNN

Data Set Link - https://www.kaggle.com/jonathanoheix/face-expression-recognition-dataset

The system we have developed is designed and programmed with 27 layers of Convolutional Neural Networks and the dataset used consists a total of 7 classes in which 28,821 images are used for training and 7,066 are used for testing. Upon training the model, the accuracy is checked and the learning rate is calculated based on the number of epochs. An epoch is a term used in machine learning to represent the number of passes of the entire training data module. As the dataset used is of large size, we divided it into a batch size of 225. Feature Extraction being the crucial part of the training phase, extracting the specific patterns from the images was done by the max pooling function. After training and extracting the features, model is created, saved, and loaded for the further testing process. The model intakes the live capturing via the default camera of the system and the output is predicted.

**Methodology:**
We proposed an Artificial Intelligence-based system that employs the concept of Deep face recognition where the dataset is taken of the same person in different angles (partially covered too) which eliminates the posed problem. All the attributes are marked for each person while training. Then this data is passed to a 4-layer Deep Neural Network[7]. So when a new live feed or video is given to the model, image formalization is done, and then facial points are considered followed by comparing the faces using a 4 layer Deep Neural Network, and after recognition, the emotions are considered for each person in the video/live feed. Here the facial emotion recognition process is done using OpenCV, Cascade Classifier, and deep neural networks. The input to the model is the live capture that is intended to get a live feed, the data to this model are photographs of each person in different angles and different emotions. When the input is passed, the model is invoked and starts processing by creating frames around each face using OpenCV, the CNN model is trained and applied to detect the emotions in the faces, haarcade classifier is used to identify the faces in the feed, then the detected faces are compared with the trained data that is given at the time of registration, and when the faces are matched and the labels are returned the focus shifts to classifying emotions of the all the persons in the frame from time-to-time in the movement[8]. The emotions are detected live on the output box. This entire process of detection takes less time which results in no community spread and faster results with an inexpensive price.
